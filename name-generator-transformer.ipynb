{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f3805ccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total names: 100\n",
      "Amount of names after removing those with unwanted characters: 100\n",
      "Using the following characters: ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "Longest name is 8 characters long\n",
      "Shortest name is 3 characters long\n",
      "131 sequences of length 5 made\n",
      "Epoch 1/10\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 32ms/step - accuracy: 0.0758 - loss: 9.8542\n",
      "Epoch 2/10\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.6815 - loss: 3.5176\n",
      "Epoch 3/10\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.7409 - loss: 3.4107\n",
      "Epoch 4/10\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.7469 - loss: 3.4306\n",
      "Epoch 5/10\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.7645 - loss: 3.1379\n",
      "Epoch 6/10\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.7547 - loss: 3.1590\n",
      "Epoch 7/10\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.7508 - loss: 3.0475\n",
      "Epoch 8/10\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.7567 - loss: 2.8089\n",
      "Epoch 9/10\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.7840 - loss: 2.7317\n",
      "Epoch 10/10\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.7469 - loss: 3.1395\n",
      "dasoe\n"
     ]
    }
   ],
   "source": [
    "#With help from ChatGPT\n",
    "\n",
    "import keras\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras import layers\n",
    "from keras.models import Model\n",
    "from keras.layers import Layer, Dense, Dropout, LayerNormalization, MultiHeadAttention\n",
    "\n",
    "\n",
    "# Predefined list of 100 names\n",
    "names_list = [\n",
    "    \"Alice\", \"Bob\", \"Charlie\", \"David\", \"Eva\", \"Fay\", \"George\", \"Hannah\", \"Ivy\", \"Jack\",\n",
    "    \"Kathy\", \"Liam\", \"Mia\", \"Noah\", \"Olivia\", \"Paul\", \"Quinn\", \"Rachel\", \"Sam\", \"Tina\",\n",
    "    \"Ursula\", \"Victor\", \"Wendy\", \"Xander\", \"Yara\", \"Zane\", \"Adam\", \"Bella\", \"Carter\", \"Daisy\",\n",
    "    \"Ella\", \"Finn\", \"Grace\", \"Henry\", \"Iris\", \"Jake\", \"Kara\", \"Leo\", \"Maya\", \"Nina\",\n",
    "    \"Oscar\", \"Piper\", \"Riley\", \"Sophie\", \"Tom\", \"Uma\", \"Vera\", \"Will\", \"Xena\", \"Yvonne\",\n",
    "    \"Zoe\", \"Amelia\", \"Benjamin\", \"Chloe\", \"Daniel\", \"Emily\", \"Frank\", \"Gina\", \"Harry\", \"Jade\",\n",
    "    \"Kevin\", \"Luna\", \"Mason\", \"Natalie\", \"Oliver\", \"Parker\", \"Quincy\", \"Rebecca\", \"Steven\", \"Tara\",\n",
    "    \"Ulysses\", \"Violet\", \"Walter\", \"Xander\", \"Yara\", \"Zane\", \"Alina\", \"Beck\", \"Cara\", \"Derek\",\n",
    "    \"Ella\", \"Felix\", \"Gwen\", \"Hugo\", \"Isla\", \"Jasper\", \"Kira\", \"Liam\", \"Molly\", \"Nico\",\n",
    "    \"Opal\", \"Pearl\", \"Quinn\", \"Rory\", \"Sage\", \"Theo\", \"Uma\", \"Vera\", \"Will\", \"Xena\",\n",
    "]\n",
    "\n",
    "def process_names(names, *, unwanted=['(', ')', '-', '.', '/']):\n",
    "    names = [name.lower() for name in names]\n",
    "    print(\"Total names:\", len(names))\n",
    "    chars = sorted(list(set(''.join(names))))\n",
    "\n",
    "    def has_unwanted(word):\n",
    "        return any(char in unwanted for char in word)\n",
    "\n",
    "    names = [name for name in names if not has_unwanted(name)]\n",
    "    print(\"Amount of names after removing those with unwanted characters:\", len(names))\n",
    "    chars = [char for char in chars if char not in unwanted]\n",
    "    print(\"Using the following characters:\", chars)\n",
    "\n",
    "    maxlen = max(len(name) for name in names)\n",
    "    minlen = min(len(name) for name in names)\n",
    "    print(\"Longest name is\", maxlen, \"characters long\")\n",
    "    print(\"Shortest name is\", minlen, \"characters long\")\n",
    "    \n",
    "    # enchar indicates the end of the word\n",
    "    endchars = '!£$%^&*()-_=+/?.>,<;:@[{}]#~'\n",
    "    endchar = [ch for ch in endchars if ch not in chars][0]\n",
    "\n",
    "    # ensures the character isn't already used & present in the training data\n",
    "    assert endchar not in chars\n",
    "    chars += [endchar]\n",
    "    \n",
    "    return names, chars\n",
    "\n",
    "names, chars = process_names(names_list)\n",
    "\n",
    "def make_sequences(names, seqlen):\n",
    "    sequences, lengths, nextchars = [], [], []\n",
    "    for name in names:\n",
    "        if len(name) <= seqlen:\n",
    "            sequences.append(name + chars[-1] * (seqlen - len(name)))\n",
    "            nextchars.append(chars[-1])\n",
    "            lengths.append(len(name))\n",
    "        else:\n",
    "            for i in range(len(name) - seqlen + 1):\n",
    "                sequences.append(name[i:i+seqlen])\n",
    "                nextchars.append(name[i + seqlen] if i + seqlen < len(name) else chars[-1])\n",
    "                lengths.append(i + seqlen)\n",
    "\n",
    "    print(len(sequences), \"sequences of length\", seqlen, \"made\")\n",
    "    \n",
    "    return sequences, lengths, nextchars\n",
    "\n",
    "seqlen = 5\n",
    "sequences, lengths, nextchars = make_sequences(names, seqlen)\n",
    "\n",
    "def make_onehots(sequences, lengths, nextchars, chars):\n",
    "    max_seq_length = max(len(seq) for seq in sequences)  # Determine max sequence length\n",
    "    vocab_size = len(chars)  # Size of the vocabulary\n",
    "\n",
    "    # Initialize arrays\n",
    "    x = np.zeros(shape=(len(sequences), max_seq_length), dtype='int32')  # Sequences\n",
    "    x2 = np.zeros(shape=(len(lengths), max(lengths)), dtype='int32')  # Lengths\n",
    "\n",
    "    for i, seq in enumerate(sequences):\n",
    "        for j, char in enumerate(seq):\n",
    "            x[i, j] = chars.index(char)\n",
    "\n",
    "    for i, l in enumerate(lengths):\n",
    "        x2[i, l-1] = 1\n",
    "\n",
    "    # Convert nextchars to integer indices for sparse categorical crossentropy\n",
    "    y = np.zeros(shape=(len(nextchars), max_seq_length), dtype='int32')  # Adjust shape\n",
    "    for i, char in enumerate(nextchars):\n",
    "        y[i] = chars.index(char)\n",
    "\n",
    "    return x, x2, y\n",
    "\n",
    "\n",
    "x, x2, y = make_onehots(sequences=sequences, lengths=lengths, nextchars=nextchars, chars=chars)\n",
    "\n",
    "@keras.saving.register_keras_serializable()\n",
    "class TransformerBlock(Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1, **kwargs):\n",
    "        super(TransformerBlock, self).__init__(**kwargs)\n",
    "        self.att = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [Dense(ff_dim, activation=\"relu\"), Dense(embed_dim)]\n",
    "        )\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = Dropout(rate)\n",
    "        self.dropout2 = Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "\n",
    "@keras.saving.register_keras_serializable()\n",
    "class TokenAndPositionEmbedding(layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim, **kwargs):\n",
    "        super(TokenAndPositionEmbedding, self).__init__(**kwargs)\n",
    "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions\n",
    "\n",
    "def create_model(input_shape, vocab_size, embed_dim, num_heads, ff_dim):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "\n",
    "    # Embedding layer\n",
    "    embedding_layer = TokenAndPositionEmbedding(input_shape[0], vocab_size, embed_dim)\n",
    "    x = embedding_layer(inputs)\n",
    "\n",
    "    # Example transformer block\n",
    "    transformer_block = TransformerBlock(embed_dim=embed_dim, num_heads=num_heads, ff_dim=ff_dim)\n",
    "    x = transformer_block(x, training=True)  # Pass 'training' argument explicitly\n",
    "\n",
    "    # Output layer with logits\n",
    "    outputs = layers.Dense(vocab_size)(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    \n",
    "    return model\n",
    "\n",
    "input_shape = (seqlen,)\n",
    "vocab_size = len(chars)\n",
    "embed_dim = 64\n",
    "num_heads = 4\n",
    "ff_dim = 128\n",
    "model = create_model(input_shape, vocab_size, embed_dim, num_heads, ff_dim)\n",
    "\n",
    "# Train the model\n",
    "model.fit(x=x, y=y, epochs=10, batch_size=64)\n",
    "\n",
    "def generate_name(model, start, *, chars, temperature=0.4):\n",
    "    maxlength = model.input_shape[1]  # Get sequence length from model input shape\n",
    "    seqlen = maxlength\n",
    "    result = start\n",
    "\n",
    "    # Prepare initial input sequence\n",
    "    sequence_input = np.zeros(shape=(1, seqlen), dtype='int32')\n",
    "    for i, char in enumerate(start):\n",
    "        sequence_input[0, i] = chars.index(char)\n",
    "\n",
    "    # Generate name\n",
    "    for _ in range(seqlen - len(start)):\n",
    "        predictions = model.predict(sequence_input)\n",
    "        predictions = predictions[0, -1, :]  # Get the predictions for the last character\n",
    "        preds = np.log(predictions) / temperature\n",
    "        exp_preds = np.exp(preds)\n",
    "        preds = exp_preds / np.sum(exp_preds)\n",
    "        next_char_index = np.argmax(np.random.multinomial(1, preds, 1))\n",
    "        next_char = chars[next_char_index]\n",
    "\n",
    "        result += next_char\n",
    "        sequence_input[0, -1] = next_char_index  # Update input sequence with predicted character\n",
    "\n",
    "    return result\n",
    "\n",
    "def sample(preds, temperature=1.0):\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds + 1e-7) / temperature\n",
    "    exp_preds = np.exp(preds - np.max(preds))\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    return np.argmax(np.random.multinomial(1, preds, 1))\n",
    "\n",
    "def get_dictchars(names,seqlen):\n",
    "    dictchars = [{} for _ in range(seqlen)]\n",
    "\n",
    "    for name in names:\n",
    "        if len(name) < seqlen:\n",
    "            continue\n",
    "        dictchars[0][name[0]] = dictchars[0].get(name[0],0) + 1\n",
    "        for i in range(1,seqlen):\n",
    "            if dictchars[i].get(name[i-1],0) == 0:\n",
    "                dictchars[i][name[i-1]] = {name[i]: 1}\n",
    "            elif dictchars[i][name[i-1]].get(name[i],0) == 0:\n",
    "                dictchars[i][name[i-1]][name[i]] = 1\n",
    "            else:\n",
    "                dictchars[i][name[i-1]][name[i]] += 1\n",
    "    return dictchars\n",
    "\n",
    "def generate_start_seq(dictchars):\n",
    "    res = \"\" # The starting sequence will be stored here\n",
    "    p = sum([n for n in dictchars[0].values()]) # total amount of letter occurences\n",
    "    r = np.random.randint(0,p) # random number used to pick the next character\n",
    "    tot = 0\n",
    "    for key, item in dictchars[0].items():\n",
    "        if r >= tot and r < tot + item:\n",
    "            res += key\n",
    "            break\n",
    "        else:\n",
    "            tot += item\n",
    "\n",
    "    for i in range(1,len(dictchars)):\n",
    "        ch = res[-1]\n",
    "        if dictchars[i].get(ch,0) == 0:\n",
    "            l = list(dictchars[i].keys())\n",
    "            ch = l[np.random.randint(0,len(l))]\n",
    "        p = sum([n for n in dictchars[i][ch].values()])\n",
    "        r = np.random.randint(0,p)\n",
    "        tot = 0\n",
    "        for key, item in dictchars[i][ch].items():\n",
    "            if r >= tot and r < tot + item:\n",
    "                res += key\n",
    "                break\n",
    "            else:\n",
    "                tot += item\n",
    "    return res\n",
    "                \n",
    "dictchars = get_dictchars(names,seqlen)\n",
    "\n",
    "def generate_random_name(model, *, chars, dictchars, temperature=0.4):\n",
    "    start = generate_start_seq(dictchars)\n",
    "    return generate_name(model, start, chars=chars, temperature=temperature)\n",
    "\n",
    "print(generate_random_name(model, chars=chars, dictchars=dictchars, temperature=0.4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "37eed0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_model(model, *, x=x, y=y, chars=chars, dictchars=dictchars, total_epochs=180, print_every=60, temperature=0.4, verbose=True):\n",
    "    for i in range(total_epochs // print_every):\n",
    "        history=model.fit(x=x, y=y, \n",
    "                            epochs=print_every,\n",
    "                            batch_size=64,\n",
    "                            validation_split=0.05,\n",
    "                            verbose=0)\n",
    "                            \n",
    "        if verbose:\n",
    "            print(\"\\nEpoch\", (i + 1) * print_every)\n",
    "            print(\"First loss:            %1.4f\" % (history.history['loss'][0]))\n",
    "            print(\"Last loss:             %1.4f\" % (history.history['loss'][-1]))\n",
    "            print(\"First validation loss: %1.4f\" % (history.history['val_loss'][0]))\n",
    "            print(\"Last validation loss:  %1.4f\" % (history.history['val_loss'][-1]))\n",
    "            print(\"\\nGenerating random names:\")\n",
    "            for _ in range(10):\n",
    "                print(generate_random_name(model, chars=chars,dictchars=dictchars, temperature=temperature))\n",
    "    if not verbose:\n",
    "        print(\"Model training complete, here are some generated names:\")\n",
    "        for _ in range(20):\n",
    "            print(generate_random_name(model, chars=chars, dictchars=dictchars, temperature=0.4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "77f13bc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 60\n",
      "First loss:            3.1805\n",
      "Last loss:             2.4625\n",
      "First validation loss: 1.2786\n",
      "Last validation loss:  1.2748\n",
      "\n",
      "Generating random names:\n",
      "wainc\n",
      "navie\n",
      "grspe\n",
      "jarey\n",
      "felle\n",
      "machi\n",
      "jally\n",
      "viphi\n",
      "dande\n",
      "catar\n",
      "\n",
      "Epoch 120\n",
      "First loss:            2.4994\n",
      "Last loss:             2.2529\n",
      "First validation loss: 1.2847\n",
      "Last validation loss:  1.1840\n",
      "\n",
      "Generating random names:\n",
      "grarg\n",
      "rilly\n",
      "olina\n",
      "quily\n",
      "viphe\n",
      "quivi\n",
      "emina\n",
      "halek\n",
      "pelto\n",
      "keloe\n",
      "\n",
      "Epoch 180\n",
      "First loss:            2.2157\n",
      "Last loss:             1.9766\n",
      "First validation loss: 1.1776\n",
      "Last validation loss:  0.8256\n",
      "\n",
      "Generating random names:\n",
      "chlix\n",
      "ulinn\n",
      "pachi\n",
      "reonn\n",
      "xarke\n",
      "wanin\n",
      "keloe\n",
      "dandy\n",
      "piona\n",
      "mophe\n"
     ]
    }
   ],
   "source": [
    "try_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "4e226b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"name_generator_model.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "6e90f70d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total names: 26\n",
      "Amount of names after removing those with unwanted characters: 26\n",
      "Using the following characters: ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "Longest name is 6 characters long\n",
      "Shortest name is 3 characters long\n",
      "28 sequences of length 5 made\n"
     ]
    }
   ],
   "source": [
    "#Finetuning example\n",
    "\n",
    "# Example of a new dataset with different names\n",
    "names_list_2 = [\n",
    "    \"Aria\", \"Blake\", \"Cody\", \"Diana\", \"Ella\", \"Finn\", \"Gavin\", \"Hazel\", \"Ivy\", \"Jack\",\n",
    "    \"Kelly\", \"Liam\", \"Mia\", \"Nora\", \"Owen\", \"Piper\", \"Quinn\", \"Riley\", \"Seth\", \"Tina\",\n",
    "    \"Uma\", \"Violet\", \"Will\", \"Xander\", \"Yara\", \"Zane\"\n",
    "]\n",
    "\n",
    "# Process the new names and prepare sequences\n",
    "names_2, chars_2 = process_names(names_list_2)\n",
    "sequences_2, lengths_2, nextchars_2 = make_sequences(names_2, seqlen)\n",
    "x_2, x2_2, y_2 = make_onehots(sequences_2, lengths_2, nextchars_2, chars_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "b3b21c25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7s/step - accuracy: 0.7714 - loss: 0.9609\n",
      "Epoch 2/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.7571 - loss: 0.9624\n",
      "Epoch 3/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - accuracy: 0.7714 - loss: 0.9617\n",
      "Epoch 4/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.7643 - loss: 0.9606\n",
      "Epoch 5/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - accuracy: 0.7714 - loss: 0.9588\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x16da31510>"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the saved model\n",
    "model_finetune = tf.keras.models.load_model(\n",
    "    \"name_generator_model.h5\", \n",
    "    custom_objects={\n",
    "        \"TokenAndPositionEmbedding\": TokenAndPositionEmbedding,\n",
    "        \"TransformerBlock\": TransformerBlock\n",
    "    },\n",
    "    compile=False  # Load the model without compiling\n",
    ")\n",
    "\n",
    "model_finetune.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),  # Or any other optimizer\n",
    "    loss='sparse_categorical_crossentropy',  # Use the appropriate loss function\n",
    "    metrics=['accuracy']  # Any metrics you want to track\n",
    ")\n",
    "\n",
    "# Fine-tune the model on the new dataset\n",
    "model_finetune.fit(x=x_2, y=y_2, epochs=5, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "13f33464",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 60\n",
      "First loss:            2.4023\n",
      "Last loss:             2.1166\n",
      "First validation loss: 0.0000\n",
      "Last validation loss:  0.0000\n",
      "\n",
      "Generating random names:\n",
      "vilte\n",
      "chlte\n",
      "gelle\n",
      "gelid\n",
      "feonk\n",
      "viorl\n",
      "urann\n",
      "dalon\n",
      "fenin\n",
      "vichi\n",
      "\n",
      "Epoch 120\n",
      "First loss:            2.1132\n",
      "Last loss:             2.0198\n",
      "First validation loss: 0.0000\n",
      "Last validation loss:  0.0000\n",
      "\n",
      "Generating random names:\n",
      "xatar\n",
      "solie\n",
      "piche\n",
      "villa\n",
      "cavin\n",
      "fevid\n",
      "kebek\n",
      "quinn\n",
      "dande\n",
      "danrl\n",
      "\n",
      "Epoch 180\n",
      "First loss:            2.0495\n",
      "Last loss:             2.0427\n",
      "First validation loss: 0.0000\n",
      "Last validation loss:  0.0000\n",
      "\n",
      "Generating random names:\n",
      "ravix\n",
      "kende\n",
      "alisy\n",
      "victo\n",
      "haili\n",
      "geace\n",
      "wavin\n",
      "dalid\n",
      "chary\n",
      "chley\n"
     ]
    }
   ],
   "source": [
    "try_model(model_finetune)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
