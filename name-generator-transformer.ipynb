{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4cba2548",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total names: 100\n",
      "Amount of names after removing those with unwanted characters: 100\n",
      "Using the following characters: ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "Longest name is 8 characters long\n",
      "Shortest name is 3 characters long\n",
      "131 sequences of length 5 made\n",
      "Epoch 1/10\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 33ms/step - accuracy: 0.1190 - loss: 9.4807\n",
      "Epoch 2/10\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.6657 - loss: 4.5917\n",
      "Epoch 3/10\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.7544 - loss: 4.0206 \n",
      "Epoch 4/10\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.7762 - loss: 3.4853\n",
      "Epoch 5/10\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.7567 - loss: 3.4279\n",
      "Epoch 6/10\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.7625 - loss: 3.2894\n",
      "Epoch 7/10\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.7684 - loss: 3.3167\n",
      "Epoch 8/10\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.7704 - loss: 3.3579\n",
      "Epoch 9/10\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.7469 - loss: 3.5087\n",
      "Epoch 10/10\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.7664 - loss: 3.1227\n",
      "olina\n"
     ]
    }
   ],
   "source": [
    "#With help from ChatGPT\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras import layers\n",
    "from keras.models import Model\n",
    "\n",
    "# Predefined list of 100 names\n",
    "names_list = [\n",
    "    \"Alice\", \"Bob\", \"Charlie\", \"David\", \"Eva\", \"Fay\", \"George\", \"Hannah\", \"Ivy\", \"Jack\",\n",
    "    \"Kathy\", \"Liam\", \"Mia\", \"Noah\", \"Olivia\", \"Paul\", \"Quinn\", \"Rachel\", \"Sam\", \"Tina\",\n",
    "    \"Ursula\", \"Victor\", \"Wendy\", \"Xander\", \"Yara\", \"Zane\", \"Adam\", \"Bella\", \"Carter\", \"Daisy\",\n",
    "    \"Ella\", \"Finn\", \"Grace\", \"Henry\", \"Iris\", \"Jake\", \"Kara\", \"Leo\", \"Maya\", \"Nina\",\n",
    "    \"Oscar\", \"Piper\", \"Riley\", \"Sophie\", \"Tom\", \"Uma\", \"Vera\", \"Will\", \"Xena\", \"Yvonne\",\n",
    "    \"Zoe\", \"Amelia\", \"Benjamin\", \"Chloe\", \"Daniel\", \"Emily\", \"Frank\", \"Gina\", \"Harry\", \"Jade\",\n",
    "    \"Kevin\", \"Luna\", \"Mason\", \"Natalie\", \"Oliver\", \"Parker\", \"Quincy\", \"Rebecca\", \"Steven\", \"Tara\",\n",
    "    \"Ulysses\", \"Violet\", \"Walter\", \"Xander\", \"Yara\", \"Zane\", \"Alina\", \"Beck\", \"Cara\", \"Derek\",\n",
    "    \"Ella\", \"Felix\", \"Gwen\", \"Hugo\", \"Isla\", \"Jasper\", \"Kira\", \"Liam\", \"Molly\", \"Nico\",\n",
    "    \"Opal\", \"Pearl\", \"Quinn\", \"Rory\", \"Sage\", \"Theo\", \"Uma\", \"Vera\", \"Will\", \"Xena\",\n",
    "]\n",
    "\n",
    "def process_names(names, *, unwanted=['(', ')', '-', '.', '/']):\n",
    "    names = [name.lower() for name in names]\n",
    "    print(\"Total names:\", len(names))\n",
    "    chars = sorted(list(set(''.join(names))))\n",
    "\n",
    "    def has_unwanted(word):\n",
    "        return any(char in unwanted for char in word)\n",
    "\n",
    "    names = [name for name in names if not has_unwanted(name)]\n",
    "    print(\"Amount of names after removing those with unwanted characters:\", len(names))\n",
    "    chars = [char for char in chars if char not in unwanted]\n",
    "    print(\"Using the following characters:\", chars)\n",
    "\n",
    "    maxlen = max(len(name) for name in names)\n",
    "    minlen = min(len(name) for name in names)\n",
    "    print(\"Longest name is\", maxlen, \"characters long\")\n",
    "    print(\"Shortest name is\", minlen, \"characters long\")\n",
    "    \n",
    "    # enchar indicates the end of the word\n",
    "    endchars = '!£$%^&*()-_=+/?.>,<;:@[{}]#~'\n",
    "    endchar = [ch for ch in endchars if ch not in chars][0]\n",
    "\n",
    "    # ensures the character isn't already used & present in the training data\n",
    "    assert endchar not in chars\n",
    "    chars += [endchar]\n",
    "    \n",
    "    return names, chars\n",
    "\n",
    "names, chars = process_names(names_list)\n",
    "\n",
    "def make_sequences(names, seqlen):\n",
    "    sequences, lengths, nextchars = [], [], []\n",
    "    for name in names:\n",
    "        if len(name) <= seqlen:\n",
    "            sequences.append(name + chars[-1] * (seqlen - len(name)))\n",
    "            nextchars.append(chars[-1])\n",
    "            lengths.append(len(name))\n",
    "        else:\n",
    "            for i in range(len(name) - seqlen + 1):\n",
    "                sequences.append(name[i:i+seqlen])\n",
    "                nextchars.append(name[i + seqlen] if i + seqlen < len(name) else chars[-1])\n",
    "                lengths.append(i + seqlen)\n",
    "\n",
    "    print(len(sequences), \"sequences of length\", seqlen, \"made\")\n",
    "    \n",
    "    return sequences, lengths, nextchars\n",
    "\n",
    "seqlen = 5\n",
    "sequences, lengths, nextchars = make_sequences(names, seqlen)\n",
    "\n",
    "def make_onehots(sequences, lengths, nextchars, chars):\n",
    "    max_seq_length = max(len(seq) for seq in sequences)  # Determine max sequence length\n",
    "    vocab_size = len(chars)  # Size of the vocabulary\n",
    "\n",
    "    # Initialize arrays\n",
    "    x = np.zeros(shape=(len(sequences), max_seq_length), dtype='int32')  # Sequences\n",
    "    x2 = np.zeros(shape=(len(lengths), max(lengths)), dtype='int32')  # Lengths\n",
    "\n",
    "    for i, seq in enumerate(sequences):\n",
    "        for j, char in enumerate(seq):\n",
    "            x[i, j] = chars.index(char)\n",
    "\n",
    "    for i, l in enumerate(lengths):\n",
    "        x2[i, l-1] = 1\n",
    "\n",
    "    # Convert nextchars to integer indices for sparse categorical crossentropy\n",
    "    y = np.zeros(shape=(len(nextchars), max_seq_length), dtype='int32')  # Adjust shape\n",
    "    for i, char in enumerate(nextchars):\n",
    "        y[i] = chars.index(char)\n",
    "\n",
    "    return x, x2, y\n",
    "\n",
    "\n",
    "x, x2, y = make_onehots(sequences=sequences, lengths=lengths, nextchars=nextchars, chars=chars)\n",
    "\n",
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super().__init__()\n",
    "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = tf.keras.Sequential(\n",
    "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim)]\n",
    "        )\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "class TokenAndPositionEmbedding(layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super().__init__()\n",
    "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions\n",
    "\n",
    "def create_model(input_shape, vocab_size, embed_dim, num_heads, ff_dim):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    \n",
    "    # Example embedding layer\n",
    "    embedding_layer = TokenAndPositionEmbedding(maxlen=input_shape[0], vocab_size=vocab_size, embed_dim=embed_dim)\n",
    "    x = embedding_layer(inputs)\n",
    "    \n",
    "    # Example transformer block\n",
    "    transformer_block = TransformerBlock(embed_dim=embed_dim, num_heads=num_heads, ff_dim=ff_dim)\n",
    "    x = transformer_block(x)\n",
    "    \n",
    "    # Output layer with logits\n",
    "    outputs = layers.Dense(vocab_size)(x)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "input_shape = (seqlen,)\n",
    "vocab_size = len(chars)\n",
    "embed_dim = 64\n",
    "num_heads = 4\n",
    "ff_dim = 128\n",
    "model = create_model(input_shape, vocab_size, embed_dim, num_heads, ff_dim)\n",
    "\n",
    "# Train the model\n",
    "model.fit(x=x, y=y, epochs=10, batch_size=64)\n",
    "\n",
    "def generate_name(model, start, *, chars, temperature=0.4):\n",
    "    maxlength = model.input_shape[1]  # Get sequence length from model input shape\n",
    "    seqlen = maxlength\n",
    "    result = start\n",
    "\n",
    "    # Prepare initial input sequence\n",
    "    sequence_input = np.zeros(shape=(1, seqlen), dtype='int32')\n",
    "    for i, char in enumerate(start):\n",
    "        sequence_input[0, i] = chars.index(char)\n",
    "\n",
    "    # Generate name\n",
    "    for _ in range(seqlen - len(start)):\n",
    "        predictions = model.predict(sequence_input)\n",
    "        predictions = predictions[0, -1, :]  # Get the predictions for the last character\n",
    "        preds = np.log(predictions) / temperature\n",
    "        exp_preds = np.exp(preds)\n",
    "        preds = exp_preds / np.sum(exp_preds)\n",
    "        next_char_index = np.argmax(np.random.multinomial(1, preds, 1))\n",
    "        next_char = chars[next_char_index]\n",
    "\n",
    "        result += next_char\n",
    "        sequence_input[0, -1] = next_char_index  # Update input sequence with predicted character\n",
    "\n",
    "    return result\n",
    "\n",
    "def sample(preds, temperature=1.0):\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds + 1e-7) / temperature\n",
    "    exp_preds = np.exp(preds - np.max(preds))\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    return np.argmax(np.random.multinomial(1, preds, 1))\n",
    "\n",
    "def get_dictchars(names,seqlen):\n",
    "    dictchars = [{} for _ in range(seqlen)]\n",
    "\n",
    "    for name in names:\n",
    "        if len(name) < seqlen:\n",
    "            continue\n",
    "        dictchars[0][name[0]] = dictchars[0].get(name[0],0) + 1\n",
    "        for i in range(1,seqlen):\n",
    "            if dictchars[i].get(name[i-1],0) == 0:\n",
    "                dictchars[i][name[i-1]] = {name[i]: 1}\n",
    "            elif dictchars[i][name[i-1]].get(name[i],0) == 0:\n",
    "                dictchars[i][name[i-1]][name[i]] = 1\n",
    "            else:\n",
    "                dictchars[i][name[i-1]][name[i]] += 1\n",
    "    return dictchars\n",
    "\n",
    "def generate_start_seq(dictchars):\n",
    "    res = \"\" # The starting sequence will be stored here\n",
    "    p = sum([n for n in dictchars[0].values()]) # total amount of letter occurences\n",
    "    r = np.random.randint(0,p) # random number used to pick the next character\n",
    "    tot = 0\n",
    "    for key, item in dictchars[0].items():\n",
    "        if r >= tot and r < tot + item:\n",
    "            res += key\n",
    "            break\n",
    "        else:\n",
    "            tot += item\n",
    "\n",
    "    for i in range(1,len(dictchars)):\n",
    "        ch = res[-1]\n",
    "        if dictchars[i].get(ch,0) == 0:\n",
    "            l = list(dictchars[i].keys())\n",
    "            ch = l[np.random.randint(0,len(l))]\n",
    "        p = sum([n for n in dictchars[i][ch].values()])\n",
    "        r = np.random.randint(0,p)\n",
    "        tot = 0\n",
    "        for key, item in dictchars[i][ch].items():\n",
    "            if r >= tot and r < tot + item:\n",
    "                res += key\n",
    "                break\n",
    "            else:\n",
    "                tot += item\n",
    "    return res\n",
    "                \n",
    "dictchars = get_dictchars(names,seqlen)\n",
    "\n",
    "def generate_random_name(model, *, chars, dictchars, temperature=0.4):\n",
    "    start = generate_start_seq(dictchars)\n",
    "    return generate_name(model, start, chars=chars, temperature=temperature)\n",
    "\n",
    "print(generate_random_name(model, chars=chars, dictchars=dictchars, temperature=0.4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b3cde09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_model(model, *, x=x, y=y, chars=chars, dictchars=dictchars, total_epochs=180, print_every=60, temperature=0.4, verbose=True):\n",
    "    for i in range(total_epochs // print_every):\n",
    "        history=model.fit(x=x, y=y, \n",
    "                            epochs=print_every,\n",
    "                            batch_size=64,\n",
    "                            validation_split=0.05,\n",
    "                            verbose=0)\n",
    "                            \n",
    "        if verbose:\n",
    "            print(\"\\nEpoch\", (i + 1) * print_every)\n",
    "            print(\"First loss:            %1.4f\" % (history.history['loss'][0]))\n",
    "            print(\"Last loss:             %1.4f\" % (history.history['loss'][-1]))\n",
    "            print(\"First validation loss: %1.4f\" % (history.history['val_loss'][0]))\n",
    "            print(\"Last validation loss:  %1.4f\" % (history.history['val_loss'][-1]))\n",
    "            print(\"\\nGenerating random names:\")\n",
    "            for _ in range(10):\n",
    "                print(generate_random_name(model, chars=chars,dictchars=dictchars, temperature=temperature))\n",
    "    if not verbose:\n",
    "        print(\"Model training complete, here are some generated names:\")\n",
    "        for _ in range(20):\n",
    "            print(generate_random_name(model, chars=chars, dictchars=dictchars, temperature=0.4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "659f1eea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 60\n",
      "First loss:            3.3403\n",
      "Last loss:             2.8575\n",
      "First validation loss: 0.2533\n",
      "Last validation loss:  0.7510\n",
      "\n",
      "Generating random names:\n",
      "grsul\n",
      "chlte\n",
      "dathi\n",
      "rainn\n",
      "renja\n",
      "ameve\n",
      "haspe\n",
      "xarke\n",
      "bella\n",
      "ulila\n",
      "\n",
      "Epoch 120\n",
      "First loss:            2.8380\n",
      "Last loss:             2.5144\n",
      "First validation loss: 0.6780\n",
      "Last validation loss:  0.0013\n",
      "\n",
      "Generating random names:\n",
      "nachy\n",
      "quice\n",
      "darke\n",
      "grsoe\n",
      "pannn\n",
      "rende\n",
      "uranc\n",
      "solon\n",
      "alisy\n",
      "frace\n",
      "\n",
      "Epoch 180\n",
      "First loss:            2.4671\n",
      "Last loss:             2.3824\n",
      "First validation loss: 0.0000\n",
      "Last validation loss:  0.0000\n",
      "\n",
      "Generating random names:\n",
      "chlon\n",
      "chace\n",
      "solli\n",
      "dannc\n",
      "quiss\n",
      "vichy\n",
      "alila\n",
      "olina\n",
      "rarke\n",
      "quily\n"
     ]
    }
   ],
   "source": [
    "try_model(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
